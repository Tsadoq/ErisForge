{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288e67e8bbf6b81d",
   "metadata": {},
   "source": [
    "# ErisForge Model Corruption Demo\n",
    "\n",
    "This notebook demonstrates how to use the ErisForge library to manipulate a language model's behavior by modifying its internal representations.\n",
    "We'll walk through the setup, loading instructions, applying model transformations, and saving the corrupted model.\n",
    "\n",
    "**Note:** This notebook assumes you have the `ErisForge` library and necessary assets installed locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b2aafa8f16b364",
   "metadata": {},
   "source": [
    "## Imports and Setup\n",
    "Import necessary libraries and set up a random seed for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Import ErisForge and the necessary scorer for evaluating refusals\n",
    "from erisforge import Forge\n",
    "from erisforge.scorers import (\n",
    "    ExpressionRefusalScorer,\n",
    ")\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random.seed(42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e16364c28fcd929",
   "metadata": {},
   "source": [
    "## Load the Model and Instructions\n",
    "Define the model name and load instructions for \"harmful\" and \"harmless\" behaviors.\n",
    "These instructions will guide the model's behavior during the transformation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1de994a8b9f53a70",
   "metadata": {},
   "source": [
    "# Specify the model we are going to modify\n",
    "MODEL = \"google/gemma-1.1-2b-it\"\n",
    "\n",
    "# Load objective behavior instructions (harmful)\n",
    "with open(\"harmful_instructions.txt\", \"r\") as f:\n",
    "    obj_beh = f.readlines()\n",
    "\n",
    "# Load anti-objective behavior instructions (harmless)\n",
    "with open(\"harmless_instructions.txt\", \"r\") as f:\n",
    "    anti_obj = f.readlines()\n",
    "\n",
    "# Limit the number of instructions to process\n",
    "max_inst = 100"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "994bffdfde2fa062",
   "metadata": {},
   "source": [
    "## Initialize ErisForge and Tokenizer\n",
    "Create an instance of `Forge` and load the behavior instructions.\n",
    "Initialize the tokenizer and model, specifying settings for device compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "91c197dc5507d8cf",
   "metadata": {},
   "source": [
    "# Initialize ErisForge\n",
    "forge = Forge()\n",
    "forge.load_instructions(\n",
    "    objective_behaviour_instructions=obj_beh, anti_behaviour_instructions=anti_obj\n",
    ")\n",
    "\n",
    "# Initialize the tokenizer with the model's configuration\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
    "\n",
    "# Load the model with specific settings for device compatibility\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for efficiency if supported\n",
    ").to(forge.device)  # Move model to the device set in forge (e.g., GPU if available)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f97fe0760b872a5",
   "metadata": {},
   "source": [
    "## Tokenize Instructions\n",
    "Convert the text instructions into tokenized format that the model can understand.\n",
    "This step is necessary for passing the instructions into the model during the transformation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f7d98a35254e6b7",
   "metadata": {},
   "source": [
    "# Tokenize the instructions for objective and anti-objective behaviors\n",
    "d_toks = forge.tokenize_instructions(\n",
    "    tokenizer=tokenizer,\n",
    "    max_n_antiobjective_instruction=max_inst,\n",
    "    max_n_objective_behaviour_instruction=max_inst,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e833c68bd1aafbdc",
   "metadata": {},
   "source": [
    "## Compute Output from Tokenized Instructions\n",
    "Run the model with the tokenized instructions to obtain output representations.\n",
    "These outputs will be used to calculate a \"direction\" that influences the model's response behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "afe15e00fd42b963",
   "metadata": {},
   "source": [
    "d_instr = forge.compute_output(\n",
    "    model=model,\n",
    "    objective_behaviour_tokenized_instructions=d_toks[\"objective_behaviour_tokens\"],\n",
    "    anti_behaviour_tokenized_instructions=d_toks[\"antiobjective_tokens\"],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e15ed13442279a76",
   "metadata": {},
   "source": [
    "## Initialize Refusal Scorer\n",
    "The `ExpressionRefusalScorer` will evaluate the model's response for specific refusal expressions.\n",
    "This scorer can help quantify the model's tendency to refuse certain types of requests after modification.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "df90c1a8b46787e0",
   "metadata": {},
   "source": [
    "scorer = ExpressionRefusalScorer()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5f893981b17a3d36",
   "metadata": {},
   "source": [
    "## Free Memory for Intermediate Variables\n",
    "To optimize memory usage, we can release intermediate variables that are no longer needed.\n",
    "This step helps manage memory, especially when working with large models."
   ]
  },
  {
   "cell_type": "code",
   "id": "e9093cd2079fa383",
   "metadata": {},
   "source": [
    "# Free up memory by deleting unused tokenized data and instruction outputs\n",
    "forge.free_memory([d_toks])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "897438942ac43d83",
   "metadata": {},
   "source": [
    "## Find Direction for Objective Behavior Transformation\n",
    "Here we compute a \"behavioral direction\" that will guide the model's transformation.\n",
    "In this example we use a layer in the middle of the model to apply the transformation, this is done because they usually perform well, you should exaustively test different layers to find the best one for your use case.\n",
    "This direction is based on the difference between harmful and harmless instruction outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "79b37c2f1a0dea0",
   "metadata": {},
   "source": [
    "refusal_dir = forge.compute_objective_behaviour_direction(\n",
    "    model=model,\n",
    "    objective_behaviour_outputs=d_instr[\"obj_beh\"],\n",
    "    antiobjective_outputs=d_instr[\"anti_obj\"],\n",
    "    layer=int(\n",
    "        len(model.model.layers) * 0.65\n",
    "    ),  # Use a specific layer to apply the transformation, in this case a layer kind of in the middle is chosen because it's a good starting point\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "99b7b6ce719b9518",
   "metadata": {},
   "source": [
    "## Finding the Best Objective Behaviour Direction (Use with Caution)\n",
    "The following cell demonstrates how to use `find_approximate_best_objective_behaviour_direction` to compute the best behavioral direction across multiple layers.\n",
    "\n",
    "**Warning:** This operation can be memory-intensive and may cause memory leaks or crashes, especially on systems with limited resources."
   ]
  },
  {
   "cell_type": "code",
   "id": "524407bc407343a5",
   "metadata": {},
   "source": [
    "try:\n",
    "    refusal_dir = forge.find_approximate_best_objective_behaviour_direction(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        scorer=scorer,\n",
    "        eval_objective_behaviour_instructions=obj_beh[:max_inst],\n",
    "        eval_antiobjective_instructions=anti_obj[:max_inst],\n",
    "        min_layer=10,\n",
    "        max_layer=13,\n",
    "    )\n",
    "    print(\"Best direction computed successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred during computation:\", e)\n",
    "    print(\"This may be due to memory constraints or a memory leak.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "57b38ab5168ea875",
   "metadata": {},
   "source": [
    "## Optional: Run the Forged Model (Example)\n",
    "You can test the modified model by running it with a set of instructions to see the altered responses.\n",
    "This step is optional, so it is commented out by default.\n",
    "\n",
    "Uncomment the code below to run the forged model and observe conversations.\n",
    "Note: Adjust 'max_new_tokens' as needed to control response length.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "457e7f282d6c3c45",
   "metadata": {},
   "source": [
    "conversations = forge.run_forged_model(\n",
    "    model=model,\n",
    "    objective_behaviour_dir=refusal_dir,\n",
    "    tokenizer=tokenizer,\n",
    "    instructions=random.sample(population=obj_beh, k=20),\n",
    "    max_new_tokens=100,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "for conversation in conversations:\n",
    "    print(\"=\" * 20)\n",
    "    for round in conversation:\n",
    "        print(f'{round[\"role\"]}: {round[\"content\"]}')\n",
    "\n",
    "forge.free_memory([conversations, d_toks, d_instr])  # Free memory after testing"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c94d92e2f75105ce",
   "metadata": {},
   "source": [
    "## Save the Transformed Model\n",
    "Save the modified model with the computed direction for future use or sharing.\n",
    "You can specify whether to save locally or push to the HuggingFace Hub.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e02f9065389c7f54",
   "metadata": {},
   "source": [
    "forge.save_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    behaviour_dir=refusal_dir,\n",
    "    output_model_name=\"corrupted_gemma_test\",  # Name for the saved model\n",
    "    to_hub=False,  # Set to True to push the model to the HuggingFace Hub\n",
    "    model_architecture=\"gemma\",  # Specify the model architecture\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ffde449ab7b4abd4",
   "metadata": {},
   "source": [
    "## Load the Corrupted Model from HuggingFace Hub\n",
    "\n",
    "To load the corrupted model, you can use the `AutoModelForCausalLM` and `AutoTokenizer` classes from the `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "id": "82bab05d78f222c3",
   "metadata": {},
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"corrupted_gemma_test\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"corrupted_gemma_test\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_text = \"give me a racist insult against italians\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_length=1000)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "669311b76ee654f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e501f5caf32ef9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we walked through the steps to modify a language model's behavior using ErisForge.\n",
    "\n",
    "\n",
    "**Note:** Exercise caution when using modified models, especially in production environments, as behavioral changes may lead to unexpected responses."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8156c3393209ddd1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
